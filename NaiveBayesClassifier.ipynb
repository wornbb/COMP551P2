{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 561kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2->pandas)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as coll\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def string_to_ngrams(s, n):\n",
    "    # text = str(s).decode('utf-8').lower()\n",
    "    text = str(s).lower()\n",
    "    text = text.replace(' ', '')\n",
    "    ngrams = nltk.ngrams([c for c in text], n)\n",
    "    return [''.join(g) for g in ngrams]\n",
    "\n",
    "class NaiveBayesClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    def __init__(self,\n",
    "                 n_gram=1,\n",
    "                 multimap=True,\n",
    "                 count_threshold=0,\n",
    "                 use_uniform_prior=False,\n",
    "                 laplace_smoothing=1.):\n",
    "        self.n_gram = n_gram\n",
    "        self.multimap = multimap\n",
    "        self.count_threshold = count_threshold\n",
    "        self.use_uniform_prior = use_uniform_prior\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        W = pd.DataFrame({'X': [X], 'y': [y]})\n",
    "        \n",
    "        # Count features\n",
    "        counts = {}\n",
    "        for (xx, yy) in zip(X, y):\n",
    "            category = np.zeros(5)\n",
    "            category[yy] = 1\n",
    "            ngrams = string_to_ngrams(xx, self.n_gram)\n",
    "            if not self.multimap:\n",
    "                ngrams = list(set(ngrams))\n",
    "            for ngram in ngrams:\n",
    "                if ngram in counts:\n",
    "                    counts[ngram] = counts[ngram] + category\n",
    "                else:\n",
    "                    counts[ngram] = category\n",
    "        counts = pd.DataFrame(counts).transpose()\n",
    "\n",
    "        # Filter low counts\n",
    "        keep = counts.apply(lambda row: sum(row) >= self.count_threshold, axis = 1)\n",
    "        counts = counts[keep == 1]\n",
    "        \n",
    "        # Apply Laplace smoothing by adding a letter\n",
    "        counts[counts.columns[-5:]] += self.laplace_smoothing\n",
    "        \n",
    "        # Count the # of n-grams observed in each language.\n",
    "        class_counts = np.array(counts[counts.columns[-5:]].apply(lambda x: np.sum(x) * 1., axis = 0).values)\n",
    "        \n",
    "        # Define P(Y = y) as the proportion of n-grams observed in each language.\n",
    "        if self.use_uniform_prior:\n",
    "            class_priors = np.full(5, .1/5)\n",
    "        else:\n",
    "            class_priors = class_counts / np.sum(class_counts)\n",
    "            \n",
    "        ## self.likelihood_ = counts.div(class_counts)\n",
    "        self.likelihood_ = np.log(counts.div(class_counts))\n",
    "        self.counts_ = counts\n",
    "        ## self.class_priors_ = class_priors\n",
    "        self.class_priors_ = np.log(class_priors)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for obs in X:\n",
    "            levels = string_to_ngrams(obs, n = self.n_gram)\n",
    "            \n",
    "            joint_likelihood = np.full(5, 1.0)\n",
    "            ## joint_likelihood = np.full(5, 0.)\n",
    "            \n",
    "            # Calculate joint probability\n",
    "            for level in levels:\n",
    "                if not level in self.likelihood_.index:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate likelihood for X\n",
    "                likelihood = self.likelihood_[self.likelihood_.index == level][:1].reset_index().values\n",
    "                likelihood = np.array(np.delete(likelihood, 0).astype(float))\n",
    "                \n",
    "                ## joint_likelihood = np.multiply(joint_likelihood, likelihood)\n",
    "                joint_likelihood = joint_likelihood + likelihood\n",
    "                \n",
    "            # Calculate joint likelihood * class prior\n",
    "            ## prop_posterior = np.multiply(joint_likelihood, self.class_priors_)\n",
    "            prop_posterior = joint_likelihood + self.class_priors_\n",
    "\n",
    "            # Calculate posterior probability\n",
    "            posterior = prop_posterior #/ np.sum(prop_posterior)\n",
    "            prediction = np.argmax(posterior)\n",
    "            \n",
    "            predictions = predictions + [prediction]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def classify(self, inputs):\n",
    "        return\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.654122</td>\n",
       "      <td>-6.384704</td>\n",
       "      <td>-6.371843</td>\n",
       "      <td>-6.502534</td>\n",
       "      <td>-9.365380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.377911</td>\n",
       "      <td>-6.131082</td>\n",
       "      <td>-6.510420</td>\n",
       "      <td>-6.349567</td>\n",
       "      <td>-9.463019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.493825</td>\n",
       "      <td>-6.298926</td>\n",
       "      <td>-6.767197</td>\n",
       "      <td>-6.723717</td>\n",
       "      <td>-9.600220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.905085</td>\n",
       "      <td>-6.761981</td>\n",
       "      <td>-7.134177</td>\n",
       "      <td>-7.169426</td>\n",
       "      <td>-9.794376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.274748</td>\n",
       "      <td>-6.984214</td>\n",
       "      <td>-7.404901</td>\n",
       "      <td>-7.789680</td>\n",
       "      <td>-10.236209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.112115</td>\n",
       "      <td>-6.610124</td>\n",
       "      <td>-7.259857</td>\n",
       "      <td>-7.504370</td>\n",
       "      <td>-9.759285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-7.457988</td>\n",
       "      <td>-6.670123</td>\n",
       "      <td>-7.434524</td>\n",
       "      <td>-7.393207</td>\n",
       "      <td>-10.236209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-7.547255</td>\n",
       "      <td>-6.413297</td>\n",
       "      <td>-7.501418</td>\n",
       "      <td>-7.764717</td>\n",
       "      <td>-10.353992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-7.615818</td>\n",
       "      <td>-7.107474</td>\n",
       "      <td>-7.559687</td>\n",
       "      <td>-8.160217</td>\n",
       "      <td>-10.418530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.963319</td>\n",
       "      <td>-6.706963</td>\n",
       "      <td>-7.452328</td>\n",
       "      <td>-7.898029</td>\n",
       "      <td>-10.487523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-2.211921</td>\n",
       "      <td>-2.619134</td>\n",
       "      <td>-2.238511</td>\n",
       "      <td>-2.665011</td>\n",
       "      <td>-2.473353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-4.037150</td>\n",
       "      <td>-4.423137</td>\n",
       "      <td>-4.313830</td>\n",
       "      <td>-3.935473</td>\n",
       "      <td>-4.059649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>-3.820184</td>\n",
       "      <td>-3.390167</td>\n",
       "      <td>-3.277092</td>\n",
       "      <td>-3.257607</td>\n",
       "      <td>-3.307399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-3.392050</td>\n",
       "      <td>-3.561555</td>\n",
       "      <td>-3.264732</td>\n",
       "      <td>-3.064531</td>\n",
       "      <td>-3.517196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>-2.384520</td>\n",
       "      <td>-2.035875</td>\n",
       "      <td>-2.070703</td>\n",
       "      <td>-1.973825</td>\n",
       "      <td>-2.531547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>-6.191544</td>\n",
       "      <td>-4.504860</td>\n",
       "      <td>-4.897719</td>\n",
       "      <td>-4.294945</td>\n",
       "      <td>-6.558503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>-6.181502</td>\n",
       "      <td>-4.773149</td>\n",
       "      <td>-4.209731</td>\n",
       "      <td>-3.758926</td>\n",
       "      <td>-4.510447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>-4.116577</td>\n",
       "      <td>-4.440494</td>\n",
       "      <td>-4.136607</td>\n",
       "      <td>-2.851689</td>\n",
       "      <td>-4.655431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>-2.891890</td>\n",
       "      <td>-2.704083</td>\n",
       "      <td>-2.921920</td>\n",
       "      <td>-2.468436</td>\n",
       "      <td>-2.515599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>-3.680520</td>\n",
       "      <td>-4.251138</td>\n",
       "      <td>-5.094542</td>\n",
       "      <td>-5.435525</td>\n",
       "      <td>-3.645602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>-3.246615</td>\n",
       "      <td>-6.230496</td>\n",
       "      <td>-5.950842</td>\n",
       "      <td>-4.297105</td>\n",
       "      <td>-3.642099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>-3.352956</td>\n",
       "      <td>-3.139286</td>\n",
       "      <td>-3.118256</td>\n",
       "      <td>-3.198637</td>\n",
       "      <td>-4.131912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>-3.020959</td>\n",
       "      <td>-3.530918</td>\n",
       "      <td>-3.510986</td>\n",
       "      <td>-3.522050</td>\n",
       "      <td>-3.331960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>-2.933299</td>\n",
       "      <td>-2.760280</td>\n",
       "      <td>-2.736345</td>\n",
       "      <td>-2.437028</td>\n",
       "      <td>-3.053857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>-2.458340</td>\n",
       "      <td>-2.805331</td>\n",
       "      <td>-2.424028</td>\n",
       "      <td>-3.423617</td>\n",
       "      <td>-2.721439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>-3.523788</td>\n",
       "      <td>-3.507671</td>\n",
       "      <td>-3.556713</td>\n",
       "      <td>-4.473972</td>\n",
       "      <td>-3.649654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>-9.460570</td>\n",
       "      <td>-4.479941</td>\n",
       "      <td>-4.400671</td>\n",
       "      <td>-8.556570</td>\n",
       "      <td>-10.353992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>-3.306319</td>\n",
       "      <td>-2.824319</td>\n",
       "      <td>-2.854490</td>\n",
       "      <td>-2.857424</td>\n",
       "      <td>-3.429256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>-3.003256</td>\n",
       "      <td>-2.579035</td>\n",
       "      <td>-2.667435</td>\n",
       "      <td>-2.777645</td>\n",
       "      <td>-3.245032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>-2.923819</td>\n",
       "      <td>-2.758829</td>\n",
       "      <td>-3.018735</td>\n",
       "      <td>-2.827553</td>\n",
       "      <td>-3.259549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.787441</td>\n",
       "      <td>-11.152793</td>\n",
       "      <td>-9.907734</td>\n",
       "      <td>-11.349778</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.867484</td>\n",
       "      <td>-12.299607</td>\n",
       "      <td>-6.826251</td>\n",
       "      <td>-10.872854</td>\n",
       "      <td>-10.418530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.954495</td>\n",
       "      <td>-11.984526</td>\n",
       "      <td>-10.742195</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-8.469588</td>\n",
       "      <td>-9.925563</td>\n",
       "      <td>-5.833487</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-5.055362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-6.801310</td>\n",
       "      <td>-7.758679</td>\n",
       "      <td>-11.053975</td>\n",
       "      <td>-10.433488</td>\n",
       "      <td>-10.641674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.299607</td>\n",
       "      <td>-11.509450</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-11.452309</td>\n",
       "      <td>-11.723024</td>\n",
       "      <td>-6.026768</td>\n",
       "      <td>-10.641674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.194247</td>\n",
       "      <td>-11.548671</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-10.841854</td>\n",
       "      <td>-9.008645</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-8.675760</td>\n",
       "      <td>-10.450059</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.082058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-5.934210</td>\n",
       "      <td>-12.040096</td>\n",
       "      <td>-7.169147</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-6.761830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-8.140724</td>\n",
       "      <td>-11.589493</td>\n",
       "      <td>-10.433488</td>\n",
       "      <td>-7.853581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.954495</td>\n",
       "      <td>-11.957858</td>\n",
       "      <td>-9.923360</td>\n",
       "      <td>-5.357064</td>\n",
       "      <td>-4.577307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-5.896514</td>\n",
       "      <td>-6.184860</td>\n",
       "      <td>-9.387649</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.641674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-5.063102</td>\n",
       "      <td>-12.504402</td>\n",
       "      <td>-11.632053</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-6.185188</td>\n",
       "      <td>-6.050028</td>\n",
       "      <td>-10.179707</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.644340</td>\n",
       "      <td>-8.721246</td>\n",
       "      <td>-4.898392</td>\n",
       "      <td>-9.576711</td>\n",
       "      <td>-10.130848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-3.865859</td>\n",
       "      <td>-3.836808</td>\n",
       "      <td>-4.129554</td>\n",
       "      <td>-4.382497</td>\n",
       "      <td>-5.047892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-4.912078</td>\n",
       "      <td>-12.822855</td>\n",
       "      <td>-8.935664</td>\n",
       "      <td>-10.774414</td>\n",
       "      <td>-3.509043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-4.312127</td>\n",
       "      <td>-9.978673</td>\n",
       "      <td>-8.881443</td>\n",
       "      <td>-11.755243</td>\n",
       "      <td>-3.256391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.504402</td>\n",
       "      <td>-11.509450</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-10.954495</td>\n",
       "      <td>-13.110537</td>\n",
       "      <td>-9.342597</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.098936</td>\n",
       "      <td>-11.877175</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-11.881872</td>\n",
       "      <td>-10.707104</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.956387</td>\n",
       "      <td>-11.271039</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.550922</td>\n",
       "      <td>-11.471710</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-9.401147</td>\n",
       "      <td>-5.403138</td>\n",
       "      <td>-8.203691</td>\n",
       "      <td>-6.724805</td>\n",
       "      <td>-5.762667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-12.011925</td>\n",
       "      <td>-11.934333</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-6.177277</td>\n",
       "      <td>-9.410960</td>\n",
       "      <td>-10.179707</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>-11.049805</td>\n",
       "      <td>-6.652069</td>\n",
       "      <td>-9.783940</td>\n",
       "      <td>-11.937565</td>\n",
       "      <td>-10.823995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4\n",
       "0   -6.654122  -6.384704  -6.371843  -6.502534  -9.365380\n",
       "1   -6.377911  -6.131082  -6.510420  -6.349567  -9.463019\n",
       "2   -6.493825  -6.298926  -6.767197  -6.723717  -9.600220\n",
       "3   -6.905085  -6.761981  -7.134177  -7.169426  -9.794376\n",
       "4   -7.274748  -6.984214  -7.404901  -7.789680 -10.236209\n",
       "5   -7.112115  -6.610124  -7.259857  -7.504370  -9.759285\n",
       "6   -7.457988  -6.670123  -7.434524  -7.393207 -10.236209\n",
       "7   -7.547255  -6.413297  -7.501418  -7.764717 -10.353992\n",
       "8   -7.615818  -7.107474  -7.559687  -8.160217 -10.418530\n",
       "9   -7.963319  -6.706963  -7.452328  -7.898029 -10.487523\n",
       "a   -2.211921  -2.619134  -2.238511  -2.665011  -2.473353\n",
       "b   -4.037150  -4.423137  -4.313830  -3.935473  -4.059649\n",
       "c   -3.820184  -3.390167  -3.277092  -3.257607  -3.307399\n",
       "d   -3.392050  -3.561555  -3.264732  -3.064531  -3.517196\n",
       "e   -2.384520  -2.035875  -2.070703  -1.973825  -2.531547\n",
       "f   -6.191544  -4.504860  -4.897719  -4.294945  -6.558503\n",
       "g   -6.181502  -4.773149  -4.209731  -3.758926  -4.510447\n",
       "h   -4.116577  -4.440494  -4.136607  -2.851689  -4.655431\n",
       "i   -2.891890  -2.704083  -2.921920  -2.468436  -2.515599\n",
       "j   -3.680520  -4.251138  -5.094542  -5.435525  -3.645602\n",
       "k   -3.246615  -6.230496  -5.950842  -4.297105  -3.642099\n",
       "l   -3.352956  -3.139286  -3.118256  -3.198637  -4.131912\n",
       "m   -3.020959  -3.530918  -3.510986  -3.522050  -3.331960\n",
       "n   -2.933299  -2.760280  -2.736345  -2.437028  -3.053857\n",
       "o   -2.458340  -2.805331  -2.424028  -3.423617  -2.721439\n",
       "p   -3.523788  -3.507671  -3.556713  -4.473972  -3.649654\n",
       "q   -9.460570  -4.479941  -4.400671  -8.556570 -10.353992\n",
       "r   -3.306319  -2.824319  -2.854490  -2.857424  -3.429256\n",
       "s   -3.003256  -2.579035  -2.667435  -2.777645  -3.245032\n",
       "t   -2.923819  -2.758829  -3.018735  -2.827553  -3.259549\n",
       "..        ...        ...        ...        ...        ...\n",
       "�  -10.787441 -11.152793  -9.907734 -11.349778 -10.823995\n",
       "�  -10.867484 -12.299607  -6.826251 -10.872854 -10.418530\n",
       "�  -10.954495 -11.984526 -10.742195 -11.937565 -10.823995\n",
       "�   -8.469588  -9.925563  -5.833487 -11.937565  -5.055362\n",
       "�   -6.801310  -7.758679 -11.053975 -10.433488 -10.641674\n",
       "�  -11.049805 -12.299607 -11.509450 -11.937565 -10.823995\n",
       "�  -11.049805 -11.452309 -11.723024  -6.026768 -10.641674\n",
       "�  -11.049805 -12.194247 -11.548671 -11.937565 -10.823995\n",
       "�  -11.049805 -10.841854  -9.008645 -11.937565 -10.823995\n",
       "�  -11.049805  -8.675760 -10.450059 -11.937565 -10.082058\n",
       "�   -5.934210 -12.040096  -7.169147 -11.937565  -6.761830\n",
       "�  -11.049805  -8.140724 -11.589493 -10.433488  -7.853581\n",
       "�  -10.954495 -11.957858  -9.923360  -5.357064  -4.577307\n",
       "�   -5.896514  -6.184860  -9.387649 -11.937565 -10.641674\n",
       "�   -5.063102 -12.504402 -11.632053 -11.937565 -10.823995\n",
       "�  -11.049805  -6.185188  -6.050028 -10.179707 -10.823995\n",
       "�  -10.644340  -8.721246  -4.898392  -9.576711 -10.130848\n",
       "�   -3.865859  -3.836808  -4.129554  -4.382497  -5.047892\n",
       "�   -4.912078 -12.822855  -8.935664 -10.774414  -3.509043\n",
       "�   -4.312127  -9.978673  -8.881443 -11.755243  -3.256391\n",
       "�  -11.049805 -12.504402 -11.509450 -11.937565 -10.823995\n",
       "�  -10.954495 -13.110537  -9.342597 -11.937565 -10.823995\n",
       "�  -11.049805 -12.098936 -11.877175 -11.937565 -10.823995\n",
       "�  -11.049805 -11.881872 -10.707104 -11.937565 -10.823995\n",
       "�  -11.049805 -12.956387 -11.271039 -11.937565 -10.823995\n",
       "�  -11.049805 -12.550922 -11.471710 -11.937565 -10.823995\n",
       "�   -9.401147  -5.403138  -8.203691  -6.724805  -5.762667\n",
       "�  -11.049805 -12.011925 -11.934333 -11.937565 -10.823995\n",
       "�  -11.049805  -6.177277  -9.410960 -10.179707 -10.823995\n",
       "�  -11.049805  -6.652069  -9.783940 -11.937565 -10.823995\n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "X_train = pd.read_csv(\"data/train_set_x.csv\")['Text'].values\n",
    "Y_train = pd.read_csv(\"data/train_set_y.csv\")['Category'].values\n",
    "X_test  = pd.read_csv(\"data/test_set_x.csv\")['Text'].values\n",
    "nbayes = NaiveBayesClassifier()\n",
    "nbayes.set_params(n_gram=1,\n",
    "                  multimap=True,\n",
    "                  count_threshold=25,\n",
    "                  use_uniform_prior=False,\n",
    "                  laplace_smoothing=10.0)\n",
    "nbayes.fit(X_train, Y_train)\n",
    "nbayes.likelihood_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276517"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nbayes.likelihood_)\n",
    "len(X_test)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.   1.   2.   1.   1.]\n",
      " [  1.  50.   5.   2.   1.]\n",
      " [  1.   4.  14.   3.   1.]\n",
      " [  1.   2.   3.   8.   1.]\n",
      " [  1.   1.   1.   1.  11.]]\n",
      "0.728\n",
      "0.272\n",
      "0.86\n",
      "0.854905660377\n"
     ]
    }
   ],
   "source": [
    "## GENERALIZED CONFUSION MATRIX\n",
    "\n",
    "true_x = X_train[250000:250100]\n",
    "true_y = Y_train[250000:250100]\n",
    "pred_y = nbayes.predict(true_x)\n",
    "\n",
    "loss = np.full((5,5), 1.0)\n",
    "for i in range(len(pred_y)):\n",
    "    loss[pred_y[i], true_y[i]] += 1\n",
    "    \n",
    "TPR = loss.trace() / loss.sum()\n",
    "FNR = 1 - TPR\n",
    "\n",
    "print(loss)\n",
    "print(TPR)\n",
    "print(FNR)\n",
    "print(accuracy_score(pred_y, true_y))\n",
    "print(precision_score(pred_y, true_y, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### RANDOM TRAIN-TEST SPLIT\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "\n",
    "xx = X_train\n",
    "yy = Y_train\n",
    "\n",
    "param_grid = [\n",
    "  {\n",
    "        'n_gram': [1, 2, 3],\n",
    "        'laplace_smoothing': [1., 10.],\n",
    "        'cv_count_threshold': [0., 10., 25.]\n",
    "  }\n",
    " ]\n",
    "\n",
    "scoring = {'acc':        'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_micro':  'recall_macro'}\n",
    "\n",
    "def train_test_split_model(model, xxx, yyy, test_size=0.004):\n",
    "    xxx_train, xxx_test, yyy_train, yyy_test = train_test_split(xxx, yyy, test_size=test_size)\n",
    "    model.fit(xxx_train, yyy_train)\n",
    "    yyy_pred = model.predict(xxx_test)\n",
    "    test_accuracy = accuracy_score(yyy_test, yyy_pred)\n",
    "    test_precision = precision_score(yyy_test, yyy_pred, average = 'macro')\n",
    "    test_recall = recall_score(yyy_test, yyy_pred, average = 'macro')\n",
    "    number_of_features = len(model.likelihood_)\n",
    "    return({\n",
    "            'n': model.n_gram,\n",
    "            'lam': model.laplace_smoothing,\n",
    "            'c': model.count_threshold,\n",
    "            'number_of_features': number_of_features,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_average': np.mean([test_accuracy, test_precision, test_recall])\n",
    "           })\n",
    "\n",
    "tts_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(train_test_split_model(NaiveBayesClassifier(n_gram=1, laplace_smoothing=10., count_threshold=25), xx, yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i_n in [1, 2, 3]:\n",
    "#    k = \"n={},lam={},c={}\".format(i_n, 1., 0)\n",
    "#    v = train_test_split_model(NaiveBayesClassifier(n_gram=i_n), xx, yy)\n",
    "#    tts_scores[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i_lam in [1., 25., 50.]:\n",
    "#    k = \"n={},lam={},c={}\".format(1, i_lam, 0)\n",
    "#    v = train_test_split_model(NaiveBayesClassifier(laplace_smoothing=i_lam), xx, yy)\n",
    "#    tts_scores[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i_c in [1, 25, 50]:\n",
    "#    k = \"n={},lam={},c={}\".format(1, 1., i_c)\n",
    "#    v = train_test_split_model(NaiveBayesClassifier(count_threshold=i_c), xx, yy)\n",
    "#    tts_scores[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1,lam=1.0,c=1\n",
      "n=1,lam=1.0,c=25\n",
      "n=1,lam=1.0,c=50\n",
      "n=1,lam=25.0,c=1\n",
      "n=1,lam=25.0,c=25\n",
      "n=1,lam=25.0,c=50\n",
      "n=1,lam=50.0,c=1\n",
      "n=1,lam=50.0,c=25\n",
      "n=1,lam=50.0,c=50\n",
      "n=2,lam=1.0,c=1\n",
      "n=2,lam=1.0,c=25\n",
      "n=2,lam=1.0,c=50\n",
      "n=2,lam=25.0,c=1\n",
      "n=2,lam=25.0,c=25\n",
      "n=2,lam=25.0,c=50\n",
      "n=2,lam=50.0,c=1\n",
      "n=2,lam=50.0,c=25\n",
      "n=2,lam=50.0,c=50\n",
      "n=3,lam=1.0,c=1\n",
      "n=3,lam=1.0,c=25\n",
      "n=3,lam=1.0,c=50\n",
      "n=3,lam=25.0,c=1\n",
      "n=3,lam=25.0,c=25\n",
      "n=3,lam=25.0,c=50\n",
      "n=3,lam=50.0,c=1\n",
      "n=3,lam=50.0,c=25\n",
      "n=3,lam=50.0,c=50\n"
     ]
    }
   ],
   "source": [
    "tts_superscore2 = {}\n",
    "\n",
    "for i_n in [1, 2, 3]:\n",
    "    for i_lam in [1., 25., 50.]:\n",
    "        for i_c in [1, 25, 50]:\n",
    "            k = \"n={},lam={},c={}\".format(i_n, i_lam, i_c)\n",
    "            print(k)\n",
    "            v = train_test_split_model(NaiveBayesClassifier(n_gram=i_n, laplace_smoothing=i_lam, count_threshold=i_c), xx, yy)\n",
    "            tts_superscore2[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### K FOLD IS TOO SLOW\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "#import random\n",
    "\n",
    "#xx = X_train[:1000]\n",
    "#yy = Y_train[:1000]\n",
    "\n",
    "#scoring = {'acc': 'accuracy',\n",
    "#           'prec_macro': 'precision_macro',\n",
    "#           'rec_micro': 'recall_macro'}\n",
    "\n",
    "#print(\"cv_n_gram\")\n",
    "#cv_n_gram = None\n",
    "#for i in range(1, 3):\n",
    "##    print(i)\n",
    "#    a = cross_validate(NaiveBayesClassifier(n_gram=i),\n",
    "#                       xx,\n",
    "#                       yy,\n",
    "#                       fit_params={},\n",
    "#                       scoring=scoring,\n",
    "#                       cv=2,\n",
    "#                       return_train_score=True)\n",
    "#    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "#    cv_n_gram = pd.concat((cv_n_gram,\n",
    "#                           a_mean.rename(\"n_gram={}\".format(i))), axis=1)\n",
    "\n",
    "#print(\"cv_laplace_smoothing\")\n",
    "#cv_laplace_smoothing = None\n",
    "#for i in [1., 5., 10.]:\n",
    "#    print(i)\n",
    "#    a = cross_validate(NaiveBayesClassifier(laplace_smoothing=i),\n",
    "#                       xx,\n",
    "#                       yy,\n",
    "#                       fit_params={},\n",
    "#                       scoring=scoring,\n",
    "#                       cv=2,\n",
    "#                       return_train_score=True)\n",
    "#    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "#    cv_laplace_smoothing = pd.concat((cv_laplace_smoothing,\n",
    "#                                      a_mean.rename(\"laplace_smoothing={}\".format(i))), axis=1)\n",
    "    \n",
    "#print(\"cv_count_threshold\")\n",
    "#cv_count_threshold = None\n",
    "#for i in [0., 10., 25.]:\n",
    "#    print(i)\n",
    "#    a = cross_validate(NaiveBayesClassifier(count_threshold=i),\n",
    "#                       xx,\n",
    "#                       yy,\n",
    "#                       fit_params={},\n",
    "#                       scoring=scoring,\n",
    "#                       cv=2,\n",
    "#                       return_train_score=True)\n",
    "#    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "#    cv_count_threshold = pd.concat((cv_count_threshold,\n",
    "#                                    a_mean.rename(\"count_threshold={}\".format(i))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>lam</th>\n",
       "      <th>n</th>\n",
       "      <th>number_of_features</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_average</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n=1,lam=1.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.872551</td>\n",
       "      <td>0.881566</td>\n",
       "      <td>0.858040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=1.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.873532</td>\n",
       "      <td>0.876884</td>\n",
       "      <td>0.880161</td>\n",
       "      <td>0.876959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=1.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.865402</td>\n",
       "      <td>0.869021</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.869701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=25.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.854562</td>\n",
       "      <td>0.855466</td>\n",
       "      <td>0.846593</td>\n",
       "      <td>0.865242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=25.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.841915</td>\n",
       "      <td>0.851823</td>\n",
       "      <td>0.854716</td>\n",
       "      <td>0.858837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=25.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.853456</td>\n",
       "      <td>0.859170</td>\n",
       "      <td>0.847539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=50.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.859079</td>\n",
       "      <td>0.863020</td>\n",
       "      <td>0.862295</td>\n",
       "      <td>0.867685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=50.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.852755</td>\n",
       "      <td>0.841529</td>\n",
       "      <td>0.838411</td>\n",
       "      <td>0.833421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1,lam=50.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.865402</td>\n",
       "      <td>0.864366</td>\n",
       "      <td>0.864952</td>\n",
       "      <td>0.862743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=1.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>0.953026</td>\n",
       "      <td>0.951320</td>\n",
       "      <td>0.947365</td>\n",
       "      <td>0.953569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=1.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>0.933153</td>\n",
       "      <td>0.926925</td>\n",
       "      <td>0.918301</td>\n",
       "      <td>0.929322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=1.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0.942186</td>\n",
       "      <td>0.937630</td>\n",
       "      <td>0.930895</td>\n",
       "      <td>0.939808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=25.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4094.0</td>\n",
       "      <td>0.915989</td>\n",
       "      <td>0.908013</td>\n",
       "      <td>0.901659</td>\n",
       "      <td>0.906391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=25.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>0.938573</td>\n",
       "      <td>0.935833</td>\n",
       "      <td>0.925546</td>\n",
       "      <td>0.943382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=25.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0.942186</td>\n",
       "      <td>0.937539</td>\n",
       "      <td>0.932074</td>\n",
       "      <td>0.938358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=50.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4082.0</td>\n",
       "      <td>0.931346</td>\n",
       "      <td>0.923758</td>\n",
       "      <td>0.919261</td>\n",
       "      <td>0.920668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=50.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2311.0</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.929445</td>\n",
       "      <td>0.928944</td>\n",
       "      <td>0.924432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=2,lam=50.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.924119</td>\n",
       "      <td>0.916179</td>\n",
       "      <td>0.898025</td>\n",
       "      <td>0.926393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=1.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56491.0</td>\n",
       "      <td>0.953930</td>\n",
       "      <td>0.953133</td>\n",
       "      <td>0.953660</td>\n",
       "      <td>0.951809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=1.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13918.0</td>\n",
       "      <td>0.971996</td>\n",
       "      <td>0.971060</td>\n",
       "      <td>0.972992</td>\n",
       "      <td>0.968191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=1.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10386.0</td>\n",
       "      <td>0.957543</td>\n",
       "      <td>0.955980</td>\n",
       "      <td>0.958117</td>\n",
       "      <td>0.952281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=25.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56504.0</td>\n",
       "      <td>0.955736</td>\n",
       "      <td>0.956078</td>\n",
       "      <td>0.963331</td>\n",
       "      <td>0.949167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=25.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13911.0</td>\n",
       "      <td>0.955736</td>\n",
       "      <td>0.951829</td>\n",
       "      <td>0.956720</td>\n",
       "      <td>0.943030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=25.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10390.0</td>\n",
       "      <td>0.960253</td>\n",
       "      <td>0.961098</td>\n",
       "      <td>0.961340</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=50.0,c=1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56476.0</td>\n",
       "      <td>0.937669</td>\n",
       "      <td>0.935771</td>\n",
       "      <td>0.954081</td>\n",
       "      <td>0.915564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=50.0,c=25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13929.0</td>\n",
       "      <td>0.960253</td>\n",
       "      <td>0.961213</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.959738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=3,lam=50.0,c=50</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10391.0</td>\n",
       "      <td>0.953930</td>\n",
       "      <td>0.950535</td>\n",
       "      <td>0.944941</td>\n",
       "      <td>0.952734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      c   lam    n  number_of_features  test_accuracy  \\\n",
       "n=1,lam=1.0,c=1     1.0   1.0  1.0               131.0       0.878049   \n",
       "n=1,lam=1.0,c=25   25.0   1.0  1.0               114.0       0.873532   \n",
       "n=1,lam=1.0,c=50   50.0   1.0  1.0               106.0       0.865402   \n",
       "n=1,lam=25.0,c=1    1.0  25.0  1.0               131.0       0.854562   \n",
       "n=1,lam=25.0,c=25  25.0  25.0  1.0               114.0       0.841915   \n",
       "n=1,lam=25.0,c=50  50.0  25.0  1.0               106.0       0.853659   \n",
       "n=1,lam=50.0,c=1    1.0  50.0  1.0               131.0       0.859079   \n",
       "n=1,lam=50.0,c=25  25.0  50.0  1.0               114.0       0.852755   \n",
       "n=1,lam=50.0,c=50  50.0  50.0  1.0               106.0       0.865402   \n",
       "n=2,lam=1.0,c=1     1.0   1.0  2.0              4094.0       0.953026   \n",
       "n=2,lam=1.0,c=25   25.0   1.0  2.0              2309.0       0.933153   \n",
       "n=2,lam=1.0,c=50   50.0   1.0  2.0              2003.0       0.942186   \n",
       "n=2,lam=25.0,c=1    1.0  25.0  2.0              4094.0       0.915989   \n",
       "n=2,lam=25.0,c=25  25.0  25.0  2.0              2309.0       0.938573   \n",
       "n=2,lam=25.0,c=50  50.0  25.0  2.0              2003.0       0.942186   \n",
       "n=2,lam=50.0,c=1    1.0  50.0  2.0              4082.0       0.931346   \n",
       "n=2,lam=50.0,c=25  25.0  50.0  2.0              2311.0       0.934959   \n",
       "n=2,lam=50.0,c=50  50.0  50.0  2.0              2004.0       0.924119   \n",
       "n=3,lam=1.0,c=1     1.0   1.0  3.0             56491.0       0.953930   \n",
       "n=3,lam=1.0,c=25   25.0   1.0  3.0             13918.0       0.971996   \n",
       "n=3,lam=1.0,c=50   50.0   1.0  3.0             10386.0       0.957543   \n",
       "n=3,lam=25.0,c=1    1.0  25.0  3.0             56504.0       0.955736   \n",
       "n=3,lam=25.0,c=25  25.0  25.0  3.0             13911.0       0.955736   \n",
       "n=3,lam=25.0,c=50  50.0  25.0  3.0             10390.0       0.960253   \n",
       "n=3,lam=50.0,c=1    1.0  50.0  3.0             56476.0       0.937669   \n",
       "n=3,lam=50.0,c=25  25.0  50.0  3.0             13929.0       0.960253   \n",
       "n=3,lam=50.0,c=50  50.0  50.0  3.0             10391.0       0.953930   \n",
       "\n",
       "                   test_average  test_precision  test_recall  \n",
       "n=1,lam=1.0,c=1        0.872551        0.881566     0.858040  \n",
       "n=1,lam=1.0,c=25       0.876884        0.880161     0.876959  \n",
       "n=1,lam=1.0,c=50       0.869021        0.871959     0.869701  \n",
       "n=1,lam=25.0,c=1       0.855466        0.846593     0.865242  \n",
       "n=1,lam=25.0,c=25      0.851823        0.854716     0.858837  \n",
       "n=1,lam=25.0,c=50      0.853456        0.859170     0.847539  \n",
       "n=1,lam=50.0,c=1       0.863020        0.862295     0.867685  \n",
       "n=1,lam=50.0,c=25      0.841529        0.838411     0.833421  \n",
       "n=1,lam=50.0,c=50      0.864366        0.864952     0.862743  \n",
       "n=2,lam=1.0,c=1        0.951320        0.947365     0.953569  \n",
       "n=2,lam=1.0,c=25       0.926925        0.918301     0.929322  \n",
       "n=2,lam=1.0,c=50       0.937630        0.930895     0.939808  \n",
       "n=2,lam=25.0,c=1       0.908013        0.901659     0.906391  \n",
       "n=2,lam=25.0,c=25      0.935833        0.925546     0.943382  \n",
       "n=2,lam=25.0,c=50      0.937539        0.932074     0.938358  \n",
       "n=2,lam=50.0,c=1       0.923758        0.919261     0.920668  \n",
       "n=2,lam=50.0,c=25      0.929445        0.928944     0.924432  \n",
       "n=2,lam=50.0,c=50      0.916179        0.898025     0.926393  \n",
       "n=3,lam=1.0,c=1        0.953133        0.953660     0.951809  \n",
       "n=3,lam=1.0,c=25       0.971060        0.972992     0.968191  \n",
       "n=3,lam=1.0,c=50       0.955980        0.958117     0.952281  \n",
       "n=3,lam=25.0,c=1       0.956078        0.963331     0.949167  \n",
       "n=3,lam=25.0,c=25      0.951829        0.956720     0.943030  \n",
       "n=3,lam=25.0,c=50      0.961098        0.961340     0.961700  \n",
       "n=3,lam=50.0,c=1       0.935771        0.954081     0.915564  \n",
       "n=3,lam=50.0,c=25      0.961213        0.963647     0.959738  \n",
       "n=3,lam=50.0,c=50      0.950535        0.944941     0.952734  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tts_superscore2).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train = pd.read_csv(\"data/train_set_x.csv\")['Text'].values\n",
    "#Y_train = pd.read_csv(\"data/train_set_y.csv\")['Category'].values\n",
    "#X_test  = pd.read_csv(\"data/test_set_x.csv\")['Text'].values\n",
    "nbayest = NaiveBayesClassifier()\n",
    "nbayest.set_params(n_gram=3,\n",
    "                   multimap=True,\n",
    "                   count_threshold=25,\n",
    "                   use_uniform_prior=False,\n",
    "                   laplace_smoothing=1.0)\n",
    "nbayest.fit(X_train, Y_train)\n",
    "pred = nbayest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
