{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 561kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2->pandas)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as coll\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def string_to_ngrams(s, n):\n",
    "    # text = str(s).decode('utf-8').lower()\n",
    "    text = str(s).lower()\n",
    "    text = text.replace(' ', '')\n",
    "    ngrams = nltk.ngrams([c for c in text], n)\n",
    "    return [''.join(g) for g in ngrams]\n",
    "\n",
    "class NaiveBayesClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    def __init__(self,\n",
    "                 n_gram=1,\n",
    "                 multimap=True,\n",
    "                 count_threshold=0,\n",
    "                 use_uniform_prior=False,\n",
    "                 laplace_smoothing=1.):\n",
    "        self.n_gram = n_gram\n",
    "        self.multimap = multimap\n",
    "        self.count_threshold = count_threshold\n",
    "        self.use_uniform_prior = use_uniform_prior\n",
    "        self.laplace_smoothing = laplace_smoothing\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        W = pd.DataFrame({'X': [X], 'y': [y]})\n",
    "        \n",
    "        # Count features\n",
    "        counts = {}\n",
    "        for (xx, yy) in zip(X, y):\n",
    "            category = np.zeros(5)\n",
    "            category[yy] = 1\n",
    "            ngrams = string_to_ngrams(xx, self.n_gram)\n",
    "            if not self.multimap:\n",
    "                ngrams = list(set(ngrams))\n",
    "            for ngram in ngrams:\n",
    "                if ngram in counts:\n",
    "                    counts[ngram] = counts[ngram] + category\n",
    "                else:\n",
    "                    counts[ngram] = category\n",
    "        counts = pd.DataFrame(counts).transpose()\n",
    "\n",
    "        # Filter low counts\n",
    "        keep = counts.apply(lambda row: sum(row) >= self.count_threshold, axis = 1)\n",
    "        counts = counts[keep == 1]\n",
    "        \n",
    "        # Apply Laplace smoothing by adding a letter\n",
    "        counts[counts.columns[-5:]] += self.laplace_smoothing\n",
    "        \n",
    "        # Count the # of n-grams observed in each language.\n",
    "        class_counts = np.array(counts[counts.columns[-5:]].apply(lambda x: np.sum(x) * 1., axis = 0).values)\n",
    "        \n",
    "        # Define P(Y = y) as the proportion of n-grams observed in each language.\n",
    "        if self.use_uniform_prior:\n",
    "            class_priors = np.full(5, .1/5)\n",
    "        else:\n",
    "            class_priors = class_counts / np.sum(class_counts)\n",
    "            \n",
    "        # self.likelihood_ = counts.div(class_counts)\n",
    "        self.likelihood_ = np.log(counts.div(class_counts)) ###\n",
    "        self.counts_ = counts\n",
    "        self.class_priors_ = class_priors\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for obs in X:\n",
    "            levels = string_to_ngrams(obs, n = self.n_gram)\n",
    "            \n",
    "            # joint_likelihood = np.full(5, 1.0)\n",
    "            joint_likelihood = np.full(5, 0.)\n",
    "            \n",
    "            # Calculate joint probability\n",
    "            for level in levels:\n",
    "                if not level in self.likelihood_.index:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate likelihood for X\n",
    "                likelihood = self.likelihood_[self.likelihood_.index == level][:1].reset_index().values\n",
    "                likelihood = np.array(np.delete(likelihood, 0).astype(float))\n",
    "                \n",
    "                # joint_likelihood = np.multiply(joint_likelihood, likelihood)\n",
    "                joint_likelihood = joint_likelihood + likelihood\n",
    "                \n",
    "            # Calculate joint likelihood * class prior\n",
    "            prop_posterior = np.multiply(joint_likelihood, self.class_priors_)\n",
    "            \n",
    "            # Calculate posterior probability\n",
    "            posterior = prop_posterior / np.sum(prop_posterior)\n",
    "            prediction = np.argmax(posterior)\n",
    "            \n",
    "            predictions = predictions + [prediction]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def classify(self, inputs):\n",
    "        return\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.114194</td>\n",
       "      <td>0.075716</td>\n",
       "      <td>0.109322</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.091656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.018406</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>0.018760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.022866</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.038695</td>\n",
       "      <td>0.039057</td>\n",
       "      <td>0.039805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.035085</td>\n",
       "      <td>0.029505</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>0.032272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.096091</td>\n",
       "      <td>0.135673</td>\n",
       "      <td>0.129296</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.086474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.001542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.011953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>0.058611</td>\n",
       "      <td>0.010339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.057854</td>\n",
       "      <td>0.069550</td>\n",
       "      <td>0.055196</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>0.087864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.028383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.040577</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.013812</td>\n",
       "      <td>0.028483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.036484</td>\n",
       "      <td>0.045008</td>\n",
       "      <td>0.045356</td>\n",
       "      <td>0.041429</td>\n",
       "      <td>0.017452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0.050849</td>\n",
       "      <td>0.030423</td>\n",
       "      <td>0.030625</td>\n",
       "      <td>0.029981</td>\n",
       "      <td>0.038839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.055507</td>\n",
       "      <td>0.065749</td>\n",
       "      <td>0.066451</td>\n",
       "      <td>0.088729</td>\n",
       "      <td>0.051292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.089253</td>\n",
       "      <td>0.062853</td>\n",
       "      <td>0.090811</td>\n",
       "      <td>0.033082</td>\n",
       "      <td>0.071518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.031139</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.028268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.038225</td>\n",
       "      <td>0.061671</td>\n",
       "      <td>0.059046</td>\n",
       "      <td>0.058276</td>\n",
       "      <td>0.035238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.051757</td>\n",
       "      <td>0.078814</td>\n",
       "      <td>0.071191</td>\n",
       "      <td>0.063116</td>\n",
       "      <td>0.042367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.056036</td>\n",
       "      <td>0.065845</td>\n",
       "      <td>0.050102</td>\n",
       "      <td>0.060043</td>\n",
       "      <td>0.041756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>️</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>�</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4\n",
       "0   0.001344  0.001753  0.001752  0.001522  0.000093\n",
       "1   0.001772  0.002259  0.001526  0.001774  0.000084\n",
       "2   0.001578  0.001910  0.001180  0.001220  0.000074\n",
       "3   0.001046  0.001202  0.000818  0.000781  0.000061\n",
       "4   0.000723  0.000963  0.000624  0.000420  0.000039\n",
       "5   0.000850  0.001399  0.000721  0.000559  0.000063\n",
       "6   0.000602  0.001318  0.000605  0.000625  0.000039\n",
       "7   0.000550  0.001704  0.000566  0.000431  0.000035\n",
       "8   0.000514  0.000851  0.000534  0.000290  0.000032\n",
       "9   0.000363  0.001270  0.000595  0.000377  0.000030\n",
       "a   0.114194  0.075716  0.109322  0.070641  0.091656\n",
       "b   0.018406  0.012466  0.013722  0.019829  0.018760\n",
       "c   0.022866  0.035021  0.038695  0.039057  0.039805\n",
       "d   0.035085  0.029505  0.039176  0.047375  0.032272\n",
       "e   0.096091  0.135673  0.129296  0.141005  0.086474\n",
       "f   0.002135  0.011488  0.007653  0.013842  0.001542\n",
       "g   0.002156  0.008784  0.015227  0.023658  0.011953\n",
       "h   0.017000  0.012251  0.016382  0.058611  0.010339\n",
       "i   0.057854  0.069550  0.055196  0.085988  0.087864\n",
       "j   0.026293  0.014805  0.006286  0.004424  0.028383\n",
       "k   0.040577  0.002045  0.002670  0.013812  0.028483\n",
       "l   0.036484  0.045008  0.045356  0.041429  0.017452\n",
       "m   0.050849  0.030423  0.030625  0.029981  0.038839\n",
       "n   0.055507  0.065749  0.066451  0.088729  0.051292\n",
       "o   0.089253  0.062853  0.090811  0.033082  0.071518\n",
       "p   0.030754  0.031139  0.029256  0.011573  0.028268\n",
       "q   0.000081  0.011777  0.012580  0.000195  0.000035\n",
       "r   0.038225  0.061671  0.059046  0.058276  0.035238\n",
       "s   0.051757  0.078814  0.071191  0.063116  0.042367\n",
       "t   0.056036  0.065845  0.050102  0.060043  0.041756\n",
       "..       ...       ...       ...       ...       ...\n",
       "�   0.000017  0.000022  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000019  0.000007  0.000022\n",
       "�   0.000017  0.000007  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000006  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000007  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000012  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000108  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000159  0.000006  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000042  0.000005  0.000007  0.000022\n",
       "�   0.000017  0.000030  0.000005  0.000007  0.000022\n",
       "�   0.000017  0.000107  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000013  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000018  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000039  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000013  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000123  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000025  0.000005  0.000007  0.000022\n",
       "�   0.000017  0.000041  0.000005  0.000007  0.000022\n",
       "�   0.000017  0.000376  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000049  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000008  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000012  0.000004  0.000007  0.000022\n",
       "�   0.000017  0.000011  0.000004  0.000007  0.000022\n",
       "   0.000017  0.000002  0.000004  0.000039  0.000022\n",
       "️   0.000017  0.000019  0.000005  0.000007  0.000022\n",
       "�   0.000017  0.002136  0.000071  0.000007  0.000022\n",
       "\n",
       "[157 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "X_train = pd.read_csv(\"data/train_set_x.csv\")['Text'].values\n",
    "Y_train = pd.read_csv(\"data/train_set_y.csv\")['Category'].values\n",
    "X_test  = pd.read_csv(\"data/test_set_x.csv\")['Text'].values\n",
    "nbayes = NaiveBayesClassifier()\n",
    "nbayes.set_params(n_gram=1,\n",
    "                  multimap=True,\n",
    "                  count_threshold=25,\n",
    "                  use_uniform_prior=False,\n",
    "                  laplace_smoothing=10.0)\n",
    "nbayes.fit(X_train, Y_train)\n",
    "nbayes.likelihood_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nbayes.likelihood_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  44.    6.    5.    1.    3.]\n",
      " [   2.  473.   46.   12.    6.]\n",
      " [   5.   28.  196.    4.    2.]\n",
      " [   1.   10.   12.  114.    2.]\n",
      " [   1.    1.    1.    2.   48.]]\n",
      "0.853658536585\n",
      "0.146341463415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:115: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "true_x = X_train[200000:201000]\n",
    "true_y = Y_train[200000:201000]\n",
    "pred_y = nbayes.predict(true_x)\n",
    "\n",
    "loss = np.full((5,5), 1.0)\n",
    "for i in range(len(pred_y)):\n",
    "    loss[pred_y[i], true_y[i]] += 1\n",
    "    \n",
    "TPR = loss.trace() / loss.sum()\n",
    "FNR = 1 - TPR\n",
    "\n",
    "print(loss)\n",
    "print(TPR)\n",
    "print(FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import random\n",
    "\n",
    "xx = X_train[:500]\n",
    "yy = Y_train[:500]\n",
    "\n",
    "\n",
    "TPR = loss.trace() / loss.sum()\n",
    "FNR = 1 - TPR\n",
    "print(TPR)\n",
    "print(FNR)\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_micro': 'recall_macro'}\n",
    "\n",
    "print(\"cv_n_gram\")\n",
    "cv_n_gram = None\n",
    "for i in range(1, 3):\n",
    "    print(i)\n",
    "    a = cross_validate(NaiveBayesClassifier(n_gram=i),\n",
    "                       xx,\n",
    "                       yy,\n",
    "                       fit_params={},\n",
    "                       scoring=scoring,\n",
    "                       cv=2,\n",
    "                       return_train_score=True)\n",
    "    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "    cv_n_gram = pd.concat((cv_n_gram,\n",
    "                           a_mean.rename(\"n_gram={}\".format(i))), axis=1)\n",
    "\n",
    "print(\"cv_laplace_smoothing\")\n",
    "cv_laplace_smoothing = None\n",
    "for i in [1., 10., 50.]:\n",
    "    print(i)\n",
    "    a = cross_validate(NaiveBayesClassifier(laplace_smoothing=i),\n",
    "                       xx,\n",
    "                       yy,\n",
    "                       fit_params={},\n",
    "                       scoring=scoring,\n",
    "                       cv=2,\n",
    "                       return_train_score=True)\n",
    "    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "    cv_laplace_smoothing = pd.concat((cv_laplace_smoothing,\n",
    "                                      a_mean.rename(\"laplace_smoothing={}\".format(i))), axis=1)\n",
    "    \n",
    "print(\"cv_count_threshold\")\n",
    "cv_count_threshold = None\n",
    "for i in [0., 10., 50.]:\n",
    "    print(i)\n",
    "    a = cross_validate(NaiveBayesClassifier(count_threshold=i),\n",
    "                       xx,\n",
    "                       yy,\n",
    "                       fit_params={},\n",
    "                       scoring=scoring,\n",
    "                       cv=2,\n",
    "                       return_train_score=True)\n",
    "    a_mean = pd.DataFrame(a).mean(axis=0)\n",
    "    cv_count_threshold = pd.concat((cv_count_threshold,\n",
    "                                    a_mean.rename(\"count_threshold={}\".format(i))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   n_gram=1   n_gram=2\n",
      "fit_time           0.028475   0.060318\n",
      "score_time        13.497908  10.938805\n",
      "test_acc           0.689676   0.860344\n",
      "test_prec_macro    0.489650   0.843353\n",
      "test_rec_micro     0.441414   0.699351\n",
      "train_acc          0.909764   0.989796\n",
      "train_prec_macro   0.923822   0.966667\n",
      "train_rec_micro    0.884933   0.996296\n",
      "                  laplace_smoothing=1.0  laplace_smoothing=10.0  \\\n",
      "fit_time                       0.021799                0.018213   \n",
      "score_time                    10.089501               11.556496   \n",
      "test_acc                       0.689676                0.550420   \n",
      "test_prec_macro                0.489650                0.210388   \n",
      "test_rec_micro                 0.441414                0.209091   \n",
      "train_acc                      0.909764                0.559824   \n",
      "train_prec_macro               0.923822                0.279438   \n",
      "train_rec_micro                0.884933                0.227249   \n",
      "\n",
      "                  laplace_smoothing=100.0  \n",
      "fit_time                         0.017361  \n",
      "score_time                       9.925259  \n",
      "test_acc                         0.540216  \n",
      "test_prec_macro                  0.108043  \n",
      "test_rec_micro                   0.200000  \n",
      "train_acc                        0.540616  \n",
      "train_prec_macro                 0.208250  \n",
      "train_rec_micro                  0.205387  \n",
      "                  count_threshold=1.0  count_threshold=10.0  \\\n",
      "fit_time                     0.018759              0.123884   \n",
      "score_time                   9.462872              9.289500   \n",
      "test_acc                     0.689676              0.689676   \n",
      "test_prec_macro              0.489650              0.646695   \n",
      "test_rec_micro               0.441414              0.548329   \n",
      "train_acc                    0.909764              0.868948   \n",
      "train_prec_macro             0.923822              0.883538   \n",
      "train_rec_micro              0.884933              0.900637   \n",
      "\n",
      "                  count_threshold=100.0  \n",
      "fit_time                       0.069804  \n",
      "score_time                     5.656569  \n",
      "test_acc                       0.599840  \n",
      "test_prec_macro                0.244048  \n",
      "test_rec_micro                 0.266919  \n",
      "train_acc                      0.640656  \n",
      "train_prec_macro               0.486559  \n",
      "train_rec_micro                0.378824  \n"
     ]
    }
   ],
   "source": [
    "print(cv_n_gram)\n",
    "print(cv_laplace_smoothing)\n",
    "print(cv_count_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
