{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as coll\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def string_to_ngrams(s, n):\n",
    "    text = str(s).decode('utf-8').lower()\n",
    "    text = text.replace(' ', '') # remove spaces\n",
    "    ngrams = nltk.ngrams([c for c in text], n)\n",
    "    return [''.join(g) for g in ngrams]\n",
    "\n",
    "class NaiveBayesClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    def __init__(self,\n",
    "                 n_gram=1,\n",
    "                 multimap=True,\n",
    "                 count_threshold=0,\n",
    "                 \n",
    "                 use_uniform_prior=False,\n",
    "                 laplace_smoothing=1.):\n",
    "        self.n_gram = n_gram\n",
    "        self.multimap = multimap\n",
    "        self.count_threshold = count_threshold\n",
    "        \n",
    "        self.use_uniform_prior = use_uniform_prior\n",
    "        self.laplace_smoothing = laplace_smoothing # Bayesian prior for Naive Bayes of laplace_smoothing / 5\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        # X, y = check_X_y(X, y)\n",
    "        \n",
    "        #\n",
    "        # Calculate frequency matrix\n",
    "        #\n",
    "        X = pd.concat([X, y], axis=1)\n",
    "        \n",
    "        Z = {}\n",
    "        # Construct hash of arrays.\n",
    "        for index, row in X.iterrows():\n",
    "            # Code the language of the observation\n",
    "            category = np.zeros(5)\n",
    "            category[row['Category']] = 1\n",
    "            # Break the text into n-grams\n",
    "            ngrams = string_to_ngrams(row['Text'], self.n_gram)\n",
    "            if not self.multimap:\n",
    "                ngrams = list(set(ngrams))\n",
    "            for ngram in ngrams:\n",
    "                if ngram in Z:\n",
    "                    # Sum element-wise with entries.\n",
    "                    Z[ngram] = Z[ngram] + category # for some reason += works by reference and glitches\n",
    "                else:\n",
    "                    Z[ngram] = category\n",
    "        \n",
    "        # Convert into data frame\n",
    "        Z = pd.DataFrame(Z).transpose()\n",
    "        \n",
    "        #\n",
    "        # Postprocessing\n",
    "        #\n",
    "        \n",
    "        # Filter low counts\n",
    "        keep = Z.apply(lambda row: sum(row) >= self.count_threshold, axis = 1)\n",
    "        Z = Z[keep == 1]\n",
    "        # Apply Laplace smoothing by adding a letter\n",
    "        Z[Z.columns[-5:]] += self.laplace_smoothing\n",
    "        # Count the # of n-grams observed in each language.\n",
    "        CY = np.array(Z[Z.columns[-5:]].apply(lambda x: np.sum(x) * 1., axis = 0).values)\n",
    "        # Define P(Y = y) as the proportion of n-grams observed in each language.\n",
    "        if self.use_uniform_prior:\n",
    "            PY = np.full(5, .1/5)\n",
    "        else:\n",
    "            PY = CY / np.sum(CY)\n",
    "            \n",
    "        self.X_ = Z\n",
    "        self.CY_ = CY\n",
    "        self.PY_ = PY\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for idx, row in X.iterrows():\n",
    "            yi = string_to_ngrams(row['Text'], n = self.n_gram)\n",
    "            # Initialize P(X1 = x1, X2 = x2, ..., Xn = xn | Y = y)\n",
    "            PX_Y = np.array([1., 1., 1., 1., 1.])\n",
    "            # Numerically stable? # PX_Y_log = np.log(PX_Y)\n",
    "            # Calculate P(X1 = x1, X2 = x2, ..., Xn = xn | Y = y) = prod_i P(Xi = xi | Y = y)\n",
    "            for yil in yi:\n",
    "                # Calculate P(X1 = x1 | Y = y)\n",
    "                if yil in self.X_.index:\n",
    "                    Px_Y = None\n",
    "                    Px_Y = self.X_[self.X_.index == yil][:1].reset_index().values\n",
    "                    Px_Y = np.delete(Px_Y, 0).astype(float)\n",
    "                    Px_Y = np.array(Px_Y)\n",
    "                else:\n",
    "                    continue # ignore unknown n-grams\n",
    "                # Obtain P(X = x | Y = y) by calculating per-category frequency\n",
    "                # of current letter\n",
    "                Px_Y = Px_Y / self.CY_\n",
    "                # Push to accumulator.\n",
    "                PX_Y = np.multiply(PX_Y, Px_Y)\n",
    "                # Numerically stable? # PX_Y_log = PX_Y_log + np.log(Px_Y)\n",
    "            # Throw in prior: P(X... | Y = y)P(Y)\n",
    "            PX_Y_PY = np.multiply(PX_Y, self.PY_)\n",
    "            # Get the posterior: P(Y|X) = P(X|Y) P(Y)/P(X)\n",
    "            # where P(X) = sum_i P(X... | Y = y_i)P(Y_i) and use\n",
    "            PY_X = PX_Y_PY / np.sum(PX_Y_PY)\n",
    "            predictions = predictions + [np.argmax(PY_X)]\n",
    "        \n",
    "        return pd.DataFrame({'Id': X['Id'],\n",
    "                             'Category': predictions},\n",
    "                            columns=['Id', 'Category'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveBayesClassifier(count_threshold=100, laplace_smoothing=10.0,\n",
       "           multimap=False, n_gram=1, use_uniform_prior=True)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "X_train = pd.read_csv(\"data/train_set_x.csv\")\n",
    "Y_train = pd.read_csv(\"data/train_set_y.csv\")\n",
    "X_test  = pd.read_csv(\"data/test_set_x.csv\")\n",
    "\n",
    "nbayes = NaiveBayesClassifier()\n",
    "nbayes.set_params(n_gram=1,\n",
    "                  multimap=False,\n",
    "                  count_threshold=100,\n",
    "                  use_uniform_prior=True,\n",
    "                  laplace_smoothing=10.)\n",
    "nbayes.fit(X_train[:3000], Y_train[:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616\n",
      "0.384\n"
     ]
    }
   ],
   "source": [
    "true_y = Y_train[3000:3100]\n",
    "pred_x = X_train[3000:3100]\n",
    "pred_y = nbayes.predict(pred_x)\n",
    "\n",
    "loss = np.full((5,5), 1.)\n",
    "\n",
    "for i in pred_y.index:\n",
    "    loss[pred_y['Category'][i], true_y['Category'][i]] += 1\n",
    "    \n",
    "TPR = loss.trace() / loss.sum()\n",
    "FNR = 1 - TPR\n",
    "\n",
    "\n",
    "\n",
    "print(loss)\n",
    "print(TPR)\n",
    "print(FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
